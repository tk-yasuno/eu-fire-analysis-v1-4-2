#!/usr/bin/env python3
"""
ç½å®³æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ  v0-6 - 200Kæ–‡æ›¸ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
Ultra-scale data collector for 200,000 wildfire-related documents
å®Ÿç”¨åŒ–å‘ã‘è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆv0-5ã®4å€ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
"""

import asyncio
import json
import logging
import multiprocessing as mp
import os
import random
import time
from concurrent.futures import ProcessPoolExecutor
from datetime import datetime
from typing import Dict, List, Set, Tuple, Any
import psutil
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class UltraScaleWildfireCollector200K:
    """200Kæ–‡æ›¸å¯¾å¿œã®è¶…å¤§è¦æ¨¡å±±ç«äº‹ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.target_documents = 200000  # 20ä¸‡æ–‡æ›¸
        self.chunk_size = 20000  # v0-5ã®2å€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        self.batch_size = 4000  # v0-5ã®2å€ãƒãƒƒãƒã‚µã‚¤ã‚º  
        self.num_workers = min(20, mp.cpu_count())  # æœ€å¤§20ãƒ¯ãƒ¼ã‚«ãƒ¼
        self.output_dir = "data_v0-6"
        self.max_retries = 15  # ãƒªãƒˆãƒ©ã‚¤å›æ•°å¢—åŠ 
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.process = psutil.Process()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.peak_memory = self.start_memory
        
        # v0-6ç”¨æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.locations = [
            # æ—¢å­˜ã®å ´æ‰€
            "California", "Oregon", "Washington", "Colorado", "Montana", "Idaho", "Wyoming",
            "Arizona", "New Mexico", "Nevada", "Utah", "Alaska", "British Columbia",
            "Alberta", "Ontario", "Quebec", "Australia", "Greece", "Spain", "Portugal",
            "Turkey", "Chile", "Argentina", "Brazil", "Russia", "Indonesia", "Philippines",
            # v0-6è¿½åŠ ï¼ˆã‚ˆã‚Šè©³ç´°ãªåœ°åŸŸï¼‰
            "Los Angeles County", "Orange County", "Riverside County", "San Bernardino County",
            "Ventura County", "Santa Barbara County", "Kern County", "Fresno County",
            "Tulare County", "Inyo County", "Mono County", "Madera County", "Merced County",
            "Stanislaus County", "San Joaquin County", "Calaveras County", "Tuolumne County",
            "Mariposa County", "Napa County", "Sonoma County", "Lake County", "Mendocino County",
            "Humboldt County", "Del Norte County", "Siskiyou County", "Modoc County",
            "Lassen County", "Plumas County", "Sierra County", "Nevada County", "Placer County",
            "El Dorado County", "Amador County", "Alpine County", "Shasta County",
            "Tehama County", "Glenn County", "Butte County", "Yuba County", "Sutter County",
            "Colusa County", "Yolo County", "Solano County", "Contra Costa County",
            "Alameda County", "San Mateo County", "Santa Clara County", "Santa Cruz County",
            "Monterey County", "San Benito County", "Kings County", "Imperial County",
            "San Diego County", "Marin County", "San Francisco County"
        ]
        
        self.fire_names = [
            # æ—¢å­˜ã®ç«ç½å
            "Camp Fire", "Thomas Fire", "Tubbs Fire", "Tunnel Fire", "Carr Fire",
            "Mendocino Fire", "Rim Fire", "Cedar Fire", "Witch Fire", "Old Fire",
            "Grand Prix Fire", "Day Fire", "Esperanza Fire", "Santiago Fire",
            "Harris Fire", "Grass Valley Fire", "Poomacha Fire", "Rice Fire",
            "Slide Fire", "King Fire", "Rocky Fire", "Valley Fire", "Butte Fire",
            "Clayton Fire", "Blue Cut Fire", "Sand Fire", "Pilot Fire", "Radford Fire",
            "Apple Fire", "El Dorado Fire", "Creek Fire", "North Fire", "Lake Fire",
            # v0-6è¿½åŠ ï¼ˆã‚ˆã‚Šå¤šæ§˜ãªç«ç½åï¼‰
            "Fairview Fire", "Mosquito Fire", "McKinney Fire", "Monument Fire",
            "Boundary Fire", "Cascade Fire", "Deadwood Fire", "Echo Fire",
            "Forest Fire", "Gateway Fire", "Highland Fire", "Iron Fire",
            "Junction Fire", "Keystone Fire", "Lightning Fire", "Meadow Fire",
            "Northern Fire", "Oak Fire", "Pacific Fire", "Ridge Fire",
            "Summit Fire", "Trinity Fire", "Union Fire", "Vista Fire",
            "Woodland Fire", "Zenith Fire", "Alpine Fire", "Boulder Fire",
            "Canyon Fire", "Desert Fire", "Eagle Fire", "Falcon Fire",
            "Granite Fire", "Harbor Fire", "Island Fire", "Jade Fire",
            "Knight Fire", "Liberty Fire", "Marine Fire", "Noble Fire",
            "Ocean Fire", "Pioneer Fire", "Quest Fire", "River Fire",
            "Stone Fire", "Timber Fire", "Urban Fire", "Valley Fire",
            "Wind Fire", "Xeric Fire", "Yield Fire", "Zero Fire",
            "Apex Fire", "Blaze Fire", "Crown Fire", "Delta Fire",
            "Ember Fire", "Flame Fire", "Glow Fire", "Heat Fire",
            "Inferno Fire", "Jet Fire", "Kindle Fire", "Lava Fire"
        ]
        
        # æ‹¡å¼µã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå¤šæ§˜æ€§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆv0-6ç”¨ï¼‰
        self.sentiment_templates = {
            "emergency_critical": [
                "CRITICAL ALERT: {} fire creates immediate life threat in {}. Emergency evacuation mandatory for zones {}.",
                "EXTREME DANGER: {} fire advancing rapidly through {} with {} mph winds. Evacuate immediately via routes {}.",
                "LIFE THREATENING: {} fire has trapped residents in {}. Emergency airlift operations underway with {}.",
                "IMMINENT THREAT: {} fire approaching {} community. All residents must evacuate now using {} corridor.",
                "RED FLAG EMERGENCY: {} fire creates firestorm conditions in {}. {} emergency shelters opened."
            ],
            "emergency_urgent": [
                "URGENT: {} fire approaching {}. Immediate evacuation required for zones {}.",
                "EMERGENCY: {} fire threatens {} in {}. Emergency services responding with {}.",
                "BREAKING: {} fire has grown to {} acres in {}. Firefighters working around the clock.",
                "ALERT: {} fire spreading rapidly through {} terrain in {}. {} crews deployed.",
                "EVACUATION: Residents of {} must leave immediately due to {} fire. Use designated routes only."
            ],
            "firefighting_operations": [
                "Firefighting crews from {} deployed to battle the massive {} fire. {} operations continuing.",
                "Containment lines holding steady against {} fire in {}. {} creating firebreaks.",
                "Helitack teams working to suppress {} fire in remote areas of {}. {} operations.",
                "Air tanker missions continue over {} fire in {}. {} drops completed today.",
                "Ground crews making progress on {} fire in {}. {}% contained as of latest update.",
                "Type 1 incident management team assuming command of {} fire in {}. {} resources assigned.",
                "Heavy air tankers conducting retardant drops on {} fire perimeter in {}. {} gallons deployed.",
                "Hot shot crews establishing control lines around {} fire in {}. {} personnel on scene."
            ],
            "weather_conditions": [
                "High winds persist across {}, elevating wildfire risk. {} fire danger declared.",
                "Temperature soaring to {} F with {}% humidity in {}. Critical fire weather conditions.",
                "Lightning strikes sparked multiple fires during dry thunderstorm in {}. {} conditions continuing.",
                "Red flag warning issued for {} due to extreme fire weather. {} winds gusting to {} mph.",
                "Rainfall finally arriving in {} after months of drought. Fire season may be ending.",
                "Atmospheric river bringing moisture to {} region. {} inches of rain forecast.",
                "Heat dome settles over {} with temperatures exceeding {} degrees. Extreme fire danger.",
                "Santa Ana winds developing across {} with gusts up to {} mph. Critical fire weather."
            ],
            "damage_assessment": [
                "Preliminary damage assessment shows {} structures destroyed by {} fire in {}.",
                "Economic impact of {} fire estimated at ${} million in {}. Insurance claims mounting.",
                "Post-fire rehabilitation begins in {} areas of {}. Erosion control priority.",
                "Reforestation project planting {} trees in burned areas of {}. {} restoration.",
                "Wildlife habitat restoration underway in fire-affected areas of {}. {} species recovery.",
                "Infrastructure damage from {} fire includes {} miles of powerlines in {}.",
                "Agricultural losses from {} fire total ${} million in {} County.",
                "Watershed impacts from {} fire affecting water quality in {} river system."
            ],
            "evacuation_shelter": [
                "Evacuation orders lifted for {} residents in {} fire area. Repopulation begins.",
                "Emergency shelters housing {} evacuees from {} fire in {}. Red Cross assistance available.",
                "Mandatory evacuation expanded to include {} neighborhoods in {}. {} routes open.",
                "Voluntary evacuation recommended for {} area residents as {} fire approaches.",
                "Evacuation center established at {} for {} fire evacuees. Capacity for {} people.",
                "Pet-friendly evacuation shelter opens at {} for {} fire displaced animals.",
                "Large animal evacuation facility activated at {} fairgrounds for {} fire.",
                "Emergency transportation provided for elderly residents in {} fire evacuation zone."
            ],
            "air_quality": [
                "Air quality alert issued for {} due to smoke from {} fire. Sensitive groups stay indoors.",
                "Smoke from {} fire creating hazardous air quality in {}. N95 masks recommended.",
                "Air quality index reaches {} in {} due to wildfire smoke. Health advisory in effect.",
                "Schools closed in {} due to poor air quality from nearby fires. Distance learning implemented.",
                "Smoke plume from {} fire visible from space. Air quality impacts felt {} miles away.",
                "Purple Air sensors show unhealthy air quality levels across {}. {} AQI reported.",
                "Respiratory health warnings issued for {} metropolitan area due to {} fire smoke.",
                "Air purification centers opened in {} to help residents escape wildfire smoke."
            ],
            "recovery_support": [
                "Community recovery center opens in {} for {} fire survivors. FEMA assistance available.",
                "Debris removal begins in fire-devastated areas of {}. {} contractor teams deployed.",
                "Utility crews restoring power to {} customers affected by {} fire in {}.",
                "Water system repairs underway in {} areas affected by {} fire.",
                "Mental health support services available for {} fire survivors. Crisis counselors on site.",
                "Temporary housing assistance provided for {} families displaced by {} fire.",
                "Small business recovery grants available for {} fire affected enterprises in {}.",
                "Volunteer cleanup efforts organized for {} fire damaged properties in {}."
            ],
            "scientific_analysis": [
                "Climate researchers studying {} fire behavior patterns in {} ecosystem.",
                "Satellite imagery reveals {} fire burn severity across {} acres in {}.",
                "Fire weather modeling predicts {} conditions for {} fire in {} region.",
                "Smoke dispersion models forecast {} air quality impacts from {} fire.",
                "Post-fire flood risk assessment conducted for {} watersheds affected by {} fire.",
                "Vegetation recovery monitoring initiated in {} fire burn areas of {}.",
                "Fire progression mapping shows {} fire spread rate of {} acres per hour.",
                "Meteorological analysis of {} fire identifies {} as primary wind driver."
            ],
            "community_resilience": [
                "Community preparedness program launched in {} following {} fire lessons learned.",
                "Defensible space inspections increase by {}% in {} after {} fire.",
                "Neighborhood fire safety coalition formed in {} to prevent future {} fire scenarios.",
                "Emergency communication system upgraded in {} following {} fire response gaps.",
                "Fire-resistant landscaping initiative begins in {} communities affected by {} fire.",
                "Evacuation route improvements planned for {} following {} fire experience.",
                "Community fire station construction approved for {} after {} fire response delays.",
                "Wildfire education program reaches {} residents in {} fire-prone areas."
            ]
        }
        
        # æ„Ÿæƒ…æ¥µæ€§ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ‹¡å¼µ
        self.sentiment_modifiers = {
            "positive": [
                "successful", "effective", "heroic", "brave", "coordinated", "swift",
                "professional", "dedicated", "tireless", "outstanding", "remarkable",
                "exceptional", "inspiring", "hopeful", "resilient", "united",
                "determined", "courageous", "skillful", "experienced"
            ],
            "negative": [
                "devastating", "catastrophic", "dangerous", "threatening", "destructive",
                "overwhelming", "tragic", "severe", "critical", "alarming",
                "unprecedented", "disastrous", "terrifying", "heartbreaking", "chaotic",
                "desperate", "exhausting", "relentless", "unforgiving", "merciless"
            ],
            "neutral": [
                "ongoing", "current", "standard", "routine", "scheduled", "planned",
                "regular", "normal", "typical", "expected", "baseline", "conventional",
                "systematic", "methodical", "procedural", "operational", "technical",
                "administrative", "logistical", "preliminary", "interim"
            ]
        }
        
        # æ‹¡å¼µçµ±è¨ˆãƒ‡ãƒ¼ã‚¿
        self.statistics_ranges = {
            "acres": [(100, 1000), (1000, 10000), (10000, 50000), (50000, 200000), (200000, 500000)],
            "structures": [(1, 50), (50, 200), (200, 1000), (1000, 5000), (5000, 20000)],
            "evacuees": [(100, 1000), (1000, 5000), (5000, 20000), (20000, 100000), (100000, 500000)],
            "personnel": [(50, 200), (200, 1000), (1000, 3000), (3000, 8000), (8000, 15000)],
            "cost_millions": [(10, 100), (100, 500), (500, 2000), (2000, 10000), (10000, 50000)],
            "temperature": [(85, 95), (95, 105), (105, 115), (115, 125), (125, 135)],
            "humidity": [(5, 15), (15, 25), (25, 40), (40, 60), (60, 85)],
            "wind_speed": [(15, 30), (30, 50), (50, 70), (70, 90), (90, 120)],
            "aqi": [(50, 100), (100, 150), (150, 200), (200, 300), (300, 500)]
        }
        
        os.makedirs(self.output_dir, exist_ok=True)
        logger.info(f"Initialized 200K Ultra-scale Collector: {self.num_workers} workers, chunk size: {self.chunk_size}")

    def monitor_memory(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–"""
        current_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        self.peak_memory = max(self.peak_memory, current_memory)
        return current_memory

    def generate_enhanced_document(self, doc_id: int) -> Dict[str, Any]:
        """å¼·åŒ–ã•ã‚ŒãŸæ–‡æ›¸ç”Ÿæˆï¼ˆã‚ˆã‚Šå¤šæ§˜æ€§ã¨ãƒªã‚¢ãƒªãƒ†ã‚£ã‚’é‡è¦–ï¼‰"""
        attempts = 0
        max_attempts = self.max_retries
        
        while attempts < max_attempts:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠï¼ˆã‚ˆã‚Šé‡ã¿ä»˜ãé¸æŠï¼‰
                category_weights = {
                    "emergency_critical": 0.05,  # 5% - æœ€é‡è¦ç·Šæ€¥
                    "emergency_urgent": 0.15,    # 15% - ç·Šæ€¥
                    "firefighting_operations": 0.25,  # 25% - æ¶ˆé˜²æ´»å‹•
                    "weather_conditions": 0.15,  # 15% - æ°—è±¡
                    "damage_assessment": 0.12,   # 12% - è¢«å®³è©•ä¾¡
                    "evacuation_shelter": 0.12,  # 12% - é¿é›£
                    "air_quality": 0.08,         # 8% - å¤§æ°—è³ª
                    "recovery_support": 0.08,    # 8% - å¾©æ—§æ”¯æ´
                    "scientific_analysis": 0.05, # 5% - ç§‘å­¦åˆ†æ
                    "community_resilience": 0.05 # 5% - åœ°åŸŸå›å¾©åŠ›
                }
                
                category = random.choices(
                    list(category_weights.keys()),
                    weights=list(category_weights.values())
                )[0]
                
                template = random.choice(self.sentiment_templates[category])
                
                # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆ
                fire_name = random.choice(self.fire_names)
                location = random.choice(self.locations)
                
                # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                stats = {}
                for stat_type, ranges in self.statistics_ranges.items():
                    selected_range = random.choice(ranges)
                    stats[stat_type] = random.randint(selected_range[0], selected_range[1])
                
                # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                params_needed = template.count('{}')
                params = []
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
                for i in range(params_needed):
                    if i == 0:
                        params.append(fire_name)
                    elif i == 1:
                        params.append(location)
                    elif i == 2:
                        # 3ç•ªç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ã¦é¸æŠ
                        if "acres" in template.lower():
                            params.append(f"{stats['acres']:,}")
                        elif "evacuee" in template.lower():
                            params.append(f"{stats['evacuees']:,}")
                        elif "temperature" in template.lower():
                            params.append(str(stats['temperature']))
                        elif "humidity" in template.lower():
                            params.append(str(stats['humidity']))
                        elif "wind" in template.lower():
                            params.append(str(stats['wind_speed']))
                        elif "personnel" in template.lower() or "crew" in template.lower():
                            params.append(f"{stats['personnel']:,}")
                        elif "cost" in template.lower() or "million" in template.lower():
                            params.append(f"{stats['cost_millions']:,}")
                        else:
                            params.append(random.choice([
                                f"Route {random.randint(1, 99)}",
                                f"Zone {random.choice(['A', 'B', 'C', 'D', 'E'])}-{random.randint(1, 20)}",
                                f"{random.randint(50, 500)} personnel",
                                f"{random.randint(10, 95)}% contained"
                            ]))
                    else:
                        # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                        options = [
                            f"Highway {random.randint(1, 101)}",
                            f"{random.randint(1, 24)} hours",
                            f"{random.choice(['north', 'south', 'east', 'west'])}bound",
                            f"{random.randint(5, 50)} structures",
                            f"Zone {random.choice(['Red', 'Orange', 'Yellow'])}"
                        ]
                        params.append(random.choice(options))
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                try:
                    text = template.format(*params)
                except (IndexError, ValueError):
                    attempts += 1
                    continue
                
                # æ„Ÿæƒ…æ¥µæ€§æ±ºå®š
                if category in ["emergency_critical", "emergency_urgent"]:
                    sentiment = "negative"
                    confidence = random.uniform(0.7, 0.95)
                elif category in ["firefighting_operations", "recovery_support", "community_resilience"]:
                    sentiment = "positive"
                    confidence = random.uniform(0.6, 0.85)
                elif category in ["scientific_analysis"]:
                    sentiment = "neutral"
                    confidence = random.uniform(0.5, 0.75)
                else:
                    # æ··åˆæ„Ÿæƒ…
                    sentiment = random.choices(
                        ["positive", "negative", "neutral"],
                        weights=[0.3, 0.5, 0.2]
                    )[0]
                    confidence = random.uniform(0.4, 0.8)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¼·åŒ–
                document = {
                    "id": f"200k_wildfire_{doc_id:06d}",
                    "text": text,
                    "sentiment": sentiment,
                    "confidence": round(confidence, 3),
                    "category": category,
                    "fire_name": fire_name,
                    "location": location,
                    "statistics": stats,
                    "timestamp": datetime.now().isoformat(),
                    "version": "v0-6_200k",
                    "word_count": len(text.split()),
                    "char_count": len(text),
                    "complexity_score": round(random.uniform(0.3, 0.9), 3)
                }
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if (len(text) >= 50 and 
                    len(text.split()) >= 8 and 
                    fire_name in text and 
                    location in text):
                    return document
                else:
                    attempts += 1
                    
            except Exception as e:
                attempts += 1
                if attempts >= max_attempts:
                    logger.warning(f"Failed to generate valid document {doc_id} after {max_attempts} attempts: {e}")
                    
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–‡æ›¸
        return {
            "id": f"200k_wildfire_{doc_id:06d}",
            "text": f"Wildfire monitoring report for {random.choice(self.fire_names)} in {random.choice(self.locations)}. Current status under assessment.",
            "sentiment": "neutral",
            "confidence": 0.5,
            "category": "fallback",
            "fire_name": random.choice(self.fire_names),
            "location": random.choice(self.locations),
            "statistics": {},
            "timestamp": datetime.now().isoformat(),
            "version": "v0-6_200k_fallback",
            "word_count": 15,
            "char_count": 100,
            "complexity_score": 0.5
        }

    def generate_batch_documents(self, worker_id: int, start_id: int, batch_size: int) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒæ–‡æ›¸ç”Ÿæˆï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼‰"""
        documents = []
        
        for i in range(batch_size):
            doc_id = start_id + i
            document = self.generate_enhanced_document(doc_id)
            documents.append(document)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºï¼ˆ1000æ–‡æ›¸ã”ã¨ï¼‰
            if (i + 1) % 1000 == 0:
                logger.info(f"Worker {worker_id}: Generated {i + 1}/{batch_size} documents")
        
        logger.info(f"Worker {worker_id} completed: {len(documents)} documents")
        return documents

    def remove_duplicates(self, all_documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡è¤‡é™¤å»ï¼ˆå¼·åŒ–ç‰ˆ - ã‚ˆã‚Šå³å¯†ãªãƒã‚§ãƒƒã‚¯ï¼‰"""
        seen_texts: Set[str] = set()
        seen_combinations: Set[Tuple[str, str, str]] = set()
        unique_documents = []
        
        for doc in all_documents:
            text = doc["text"]
            fire_name = doc.get("fire_name", "")
            location = doc.get("location", "")
            
            # ãƒ†ã‚­ã‚¹ãƒˆé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if text in seen_texts:
                continue
            
            # çµ„ã¿åˆã‚ã›é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆç«ç½å+å ´æ‰€+ã‚«ãƒ†ã‚´ãƒªï¼‰
            combination = (fire_name, location, doc.get("category", ""))
            if combination in seen_combinations:
                continue
            
            seen_texts.add(text)
            seen_combinations.add(combination)
            unique_documents.append(doc)
        
        logger.info(f"Duplicate removal: {len(all_documents)} -> {len(unique_documents)} documents")
        return unique_documents

    def save_chunked_data(self, documents: List[Dict[str, Any]], timestamp: str) -> List[str]:
        """ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¿å­˜ï¼ˆ200Kå¯¾å¿œï¼‰"""
        chunk_files = []
        
        for i in range(0, len(documents), self.chunk_size):
            chunk = documents[i:i + self.chunk_size]
            chunk_num = i // self.chunk_size + 1
            
            chunk_filename = f"{timestamp}_wildfire_v0-6_200k_chunk_{chunk_num:02d}.json"
            chunk_path = os.path.join(self.output_dir, chunk_filename)
            
            with open(chunk_path, 'w', encoding='utf-8') as f:
                json.dump(chunk, f, indent=2, ensure_ascii=False)
            
            chunk_files.append(chunk_filename)
            logger.info(f"Chunk {chunk_num} saved: {len(chunk)} documents to {chunk_filename}")
        
        return chunk_files

    async def collect_200k_data(self) -> Dict[str, Any]:
        """200Kæ–‡æ›¸åé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        start_time = time.time()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info("=== 200K Ultra-scale Data Collection Started ===")
        logger.info(f"Target: {self.target_documents:,} documents")
        logger.info(f"Workers: {self.num_workers}")
        logger.info(f"Batch size: {self.batch_size}")
        logger.info(f"Chunk size: {self.chunk_size}")
        
        all_documents = []
        
        # ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            futures = []
            
            current_id = 0
            while current_id < self.target_documents:
                remaining = self.target_documents - current_id
                batch_size = min(self.batch_size, remaining)
                
                future = executor.submit(
                    self.generate_batch_documents,
                    len(futures),  # worker_id
                    current_id,
                    batch_size
                )
                futures.append(future)
                current_id += batch_size
            
            # çµæœåé›†
            with tqdm(total=len(futures), desc="Processing batches") as pbar:
                for future in futures:
                    batch_documents = future.result()
                    all_documents.extend(batch_documents)
                    pbar.update(1)
                    
                    # ãƒ¡ãƒ¢ãƒªç›£è¦–
                    current_memory = self.monitor_memory()
                    if current_memory > 2000:  # 2GBè¶…éã§è­¦å‘Š
                        logger.warning(f"High memory usage: {current_memory:.1f} MB")
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†
        logger.info(f"Integrating {len(all_documents)} documents...")
        
        # é‡è¤‡é™¤å»
        unique_documents = self.remove_duplicates(all_documents)
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¿å­˜
        chunk_files = self.save_chunked_data(unique_documents, timestamp)
        
        # åé›†çµæœä¿å­˜
        processing_time = time.time() - start_time
        final_memory = self.monitor_memory()
        
        collection_results = {
            "version": "v0-6_200k",
            "timestamp": timestamp,
            "target_documents": self.target_documents,
            "generated_documents": len(all_documents),
            "unique_documents": len(unique_documents),
            "duplicate_rate": round((len(all_documents) - len(unique_documents)) / len(all_documents) * 100, 2),
            "processing_time_seconds": round(processing_time, 2),
            "processing_time_minutes": round(processing_time / 60, 2),
            "speed_docs_per_second": round(len(unique_documents) / processing_time, 1),
            "num_workers": self.num_workers,
            "batch_size": self.batch_size,
            "chunk_size": self.chunk_size,
            "num_chunks": len(chunk_files),
            "chunk_files": chunk_files,
            "memory_usage": {
                "start_mb": round(self.start_memory, 1),
                "peak_mb": round(self.peak_memory, 1),
                "final_mb": round(final_memory, 1)
            },
            "data_quality": {
                "avg_text_length": round(sum(len(doc["text"]) for doc in unique_documents) / len(unique_documents), 1),
                "avg_word_count": round(sum(doc.get("word_count", 0) for doc in unique_documents) / len(unique_documents), 1),
                "categories": len(set(doc["category"] for doc in unique_documents)),
                "sentiment_distribution": {
                    sentiment: sum(1 for doc in unique_documents if doc["sentiment"] == sentiment)
                    for sentiment in ["positive", "negative", "neutral"]
                }
            }
        }
        
        results_path = os.path.join(self.output_dir, f"collection_results_{timestamp}.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(collection_results, f, indent=2, ensure_ascii=False)
        
        # çµæœè¡¨ç¤º
        logger.info("200K Ultra-scale data collection completed!")
        logger.info(f"Generated: {len(unique_documents):,} documents")
        logger.info(f"Processing time: {processing_time:.2f} seconds")
        logger.info(f"Speed: {collection_results['speed_docs_per_second']} docs/sec")
        logger.info(f"Peak memory: {self.peak_memory:.1f} MB")
        logger.info(f"Chunks created: {len(chunk_files)}")
        
        print(f"\nğŸ‰ 200K Ultra-scale data collection completed!")
        print(f"ğŸ“Š Total documents: {len(unique_documents):,}")
        print(f"â±ï¸  Processing time: {processing_time:.2f} seconds")
        print(f"ğŸš€ Speed: {collection_results['speed_docs_per_second']} docs/sec")
        print(f"ğŸ’¾ Peak memory: {self.peak_memory:.1f} MB")
        print(f"ğŸ“ Chunks created: {len(chunk_files)}")
        print(f"ğŸ“„ Results saved: {results_path}")
        
        return collection_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    collector = UltraScaleWildfireCollector200K()
    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(collector.collect_200k_data())
    return results

if __name__ == "__main__":
    main()