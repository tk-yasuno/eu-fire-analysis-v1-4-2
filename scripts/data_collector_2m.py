#!/usr/bin/env python3
"""
ç½å®³æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ  - 200ä¸‡æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ for v1-0ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
2M Document Collector for Scaling Test (v0-7 based)
v1-0é–‹ç™ºç”¨ã®2å€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
"""

import gc
import json
import logging
import os
import random
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Any
import psutil
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Data2MConfig:
    """200ä¸‡æ–‡æ›¸å¯¾å¿œãƒ‡ãƒ¼ã‚¿ç”Ÿæˆè¨­å®š"""
    target_documents: int = 2_000_000  # 200ä¸‡æ–‡æ›¸
    batch_size: int = 10000  # 1ä¸‡æ–‡æ›¸/ãƒãƒƒãƒ
    chunk_size: int = 100000  # 10ä¸‡æ–‡æ›¸/ãƒãƒ£ãƒ³ã‚¯ï¼ˆv0-7ã¨åŒã‚µã‚¤ã‚ºï¼‰
    workers: int = 16  # v0-7ã®12ã‹ã‚‰å¢—å¼·
    output_dir: str = "data_v1-0"
    max_memory_gb: float = 12.0  # ãƒ¡ãƒ¢ãƒªåˆ¶é™
    
    # å“è³ªè¨­å®šï¼ˆv0-7ãƒ™ãƒ¼ã‚¹æ”¹è‰¯ï¼‰
    sentiments: List[str] = None
    locations: List[str] = None
    fire_types: List[str] = None
    
    def __post_init__(self):
        # v0-7ã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸæ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªï¼ˆ14ç¨®é¡ï¼‰
        self.sentiments = [
            "ä¸å®‰", "ææ€–", "æ€’ã‚Š", "æ‚²ã—ã¿", "çµ¶æœ›", 
            "å¸Œæœ›", "å®‰å µ", "æ„Ÿè¬", "é€£å¸¯", "æ±ºæ„",
            "å›°æƒ‘", "ç„¦ç‡¥", "è­¦æˆ’", "å†·é™"  # 2ç¨®é¡è¿½åŠ 
        ]
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ç½å®³ç™ºç”Ÿåœ°ç‚¹ï¼ˆv0-7ã®153ã‹ã‚‰200ç®‡æ‰€ã«æ‹¡å¼µï¼‰
        self.locations = [
            # æ—¥æœ¬ï¼ˆè©³ç´°åŒ–ï¼‰
            "åŒ—æµ·é“", "é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ",
            "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ",
            "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ", "å²é˜œçœŒ",
            "é™å²¡çœŒ", "æ„›çŸ¥çœŒ", "ä¸‰é‡çœŒ", "æ»‹è³€çœŒ", "äº¬éƒ½åºœ", "å¤§é˜ªåºœ", "å…µåº«çœŒ",
            "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ", "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å²¡å±±çœŒ", "åºƒå³¶çœŒ", "å±±å£çœŒ",
            "å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ", "ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ",
            "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ",
            
            # åŒ—ç±³ï¼ˆæ‹¡å¼µï¼‰
            "ã‚«ãƒªãƒ•ã‚©ãƒ«ãƒ‹ã‚¢å·", "ãƒ†ã‚­ã‚µã‚¹å·", "ãƒ•ãƒ­ãƒªãƒ€å·", "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯å·", "ãƒšãƒ³ã‚·ãƒ«ãƒ™ãƒ‹ã‚¢å·",
            "ã‚¤ãƒªãƒã‚¤å·", "ã‚ªãƒã‚¤ã‚ªå·", "ã‚¸ãƒ§ãƒ¼ã‚¸ã‚¢å·", "ãƒãƒ¼ã‚¹ã‚«ãƒ­ãƒ©ã‚¤ãƒŠå·", "ãƒŸã‚·ã‚¬ãƒ³å·",
            "ãƒ‹ãƒ¥ãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¸ãƒ¼å·", "ãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·", "ãƒ¯ã‚·ãƒ³ãƒˆãƒ³å·", "ã‚¢ãƒªã‚¾ãƒŠå·", "ãƒã‚µãƒãƒ¥ãƒ¼ã‚»ãƒƒãƒ„å·",
            "ãƒ†ãƒã‚·ãƒ¼å·", "ã‚¤ãƒ³ãƒ‡ã‚£ã‚¢ãƒŠå·", "ãƒ¡ãƒªãƒ¼ãƒ©ãƒ³ãƒ‰å·", "ãƒŸã‚ºãƒ¼ãƒªå·", "ã‚¦ã‚£ã‚¹ã‚³ãƒ³ã‚·ãƒ³å·",
            "ã‚³ãƒ­ãƒ©ãƒ‰å·", "ãƒŸãƒã‚½ã‚¿å·", "ã‚µã‚¦ã‚¹ã‚«ãƒ­ãƒ©ã‚¤ãƒŠå·", "ã‚¢ãƒ©ãƒãƒå·", "ãƒ«ã‚¤ã‚¸ã‚¢ãƒŠå·",
            "ã‚±ãƒ³ã‚¿ãƒƒã‚­ãƒ¼å·", "ã‚ªãƒ¬ã‚´ãƒ³å·", "ã‚ªã‚¯ãƒ©ãƒ›ãƒå·", "ã‚³ãƒãƒã‚«ãƒƒãƒˆå·", "ãƒ¦ã‚¿å·",
            
            # ã‚«ãƒŠãƒ€
            "ãƒ–ãƒªãƒ†ã‚£ãƒƒã‚·ãƒ¥ã‚³ãƒ­ãƒ³ãƒ“ã‚¢å·", "ã‚¢ãƒ«ãƒãƒ¼ã‚¿å·", "ã‚µã‚¹ã‚«ãƒãƒ¥ãƒ¯ãƒ³å·", "ãƒãƒ‹ãƒˆãƒå·",
            "ã‚ªãƒ³ã‚¿ãƒªã‚ªå·", "ã‚±ãƒ™ãƒƒã‚¯å·", "ãƒ‹ãƒ¥ãƒ¼ãƒ–ãƒ©ãƒ³ã‚ºã‚¦ã‚£ãƒƒã‚¯å·", "ãƒãƒã‚¹ã‚³ã‚·ã‚¢å·",
            "ãƒ—ãƒªãƒ³ã‚¹ã‚¨ãƒ‰ãƒ¯ãƒ¼ãƒ‰ã‚¢ã‚¤ãƒ©ãƒ³ãƒ‰å·", "ãƒ‹ãƒ¥ãƒ¼ãƒ•ã‚¡ãƒ³ãƒ‰ãƒ©ãƒ³ãƒ‰ãƒ»ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«å·",
            
            # ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢
            "ãƒ‹ãƒ¥ãƒ¼ã‚µã‚¦ã‚¹ã‚¦ã‚§ãƒ¼ãƒ«ã‚ºå·", "ãƒ“ã‚¯ãƒˆãƒªã‚¢å·", "ã‚¯ã‚¤ãƒ¼ãƒ³ã‚ºãƒ©ãƒ³ãƒ‰å·", "è¥¿ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢å·",
            "å—ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢å·", "ã‚¿ã‚¹ãƒãƒ‹ã‚¢å·", "ãƒãƒ¼ã‚¶ãƒ³ãƒ†ãƒªãƒˆãƒªãƒ¼", "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢é¦–éƒ½ç‰¹åˆ¥åœ°åŸŸ",
            
            # ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ï¼ˆä¸»è¦å›½ï¼‰
            "ãƒ‰ã‚¤ãƒ„ãƒ»ãƒã‚¤ã‚¨ãƒ«ãƒ³å·", "ãƒ‰ã‚¤ãƒ„ãƒ»ãƒãƒ«ãƒˆãƒ©ã‚¤ãƒ³ï¼ãƒ´ã‚§ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ãƒ¬ãƒ³å·", "ãƒ•ãƒ©ãƒ³ã‚¹ãƒ»ãƒ—ãƒ­ãƒ´ã‚¡ãƒ³ã‚¹",
            "ã‚¹ãƒšã‚¤ãƒ³ãƒ»ã‚¢ãƒ³ãƒ€ãƒ«ã‚·ã‚¢å·", "ã‚¤ã‚¿ãƒªã‚¢ãƒ»ã‚·ãƒãƒªã‚¢å³¶", "ã‚¤ã‚¿ãƒªã‚¢ãƒ»ãƒˆã‚¹ã‚«ãƒ¼ãƒŠå·",
            "ã‚®ãƒªã‚·ãƒ£ãƒ»ã‚¢ãƒƒãƒ†ã‚£ã‚«å·", "ãƒãƒ«ãƒˆã‚¬ãƒ«ãƒ»ã‚¢ãƒ¬ãƒ³ãƒ†ãƒ¼ã‚¸ãƒ§å·", "è‹±å›½ãƒ»ã‚¹ã‚³ãƒƒãƒˆãƒ©ãƒ³ãƒ‰",
            "è‹±å›½ãƒ»ã‚¦ã‚§ãƒ¼ãƒ«ã‚º", "ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³ãƒ»ã‚¤ã‚§ãƒ¼ã‚¿ãƒ©ãƒ³ãƒ‰åœ°æ–¹", "ãƒãƒ«ã‚¦ã‚§ãƒ¼ãƒ»å—ãƒãƒ«ã‚¦ã‚§ãƒ¼",
            
            # å—ç±³
            "ãƒ–ãƒ©ã‚¸ãƒ«ãƒ»ã‚¢ãƒã‚¾ãƒŠã‚¹å·", "ãƒ–ãƒ©ã‚¸ãƒ«ãƒ»ã‚µãƒ³ãƒ‘ã‚¦ãƒ­å·", "ãƒ–ãƒ©ã‚¸ãƒ«ãƒ»ãƒªã‚ªãƒ‡ã‚¸ãƒ£ãƒã‚¤ãƒ­å·",
            "ã‚¢ãƒ«ã‚¼ãƒ³ãƒãƒ³ãƒ»ãƒ–ã‚¨ãƒã‚¹ã‚¢ã‚¤ãƒ¬ã‚¹å·", "ãƒãƒªãƒ»ã‚µãƒ³ãƒ†ã‚£ã‚¢ã‚´é¦–éƒ½å·", "ãƒšãƒ«ãƒ¼ãƒ»ãƒªãƒå·",
            "ã‚³ãƒ­ãƒ³ãƒ“ã‚¢ãƒ»ã‚¢ãƒ³ãƒ†ã‚£ã‚ªã‚­ã‚¢çœŒ", "ãƒ™ãƒã‚ºã‚¨ãƒ©ãƒ»ã‚«ãƒ©ã‚«ã‚¹é¦–éƒ½åœ°åŒº",
            
            # ã‚¢ã‚¸ã‚¢ï¼ˆæ‹¡å¼µï¼‰
            "ä¸­å›½ãƒ»å››å·çœ", "ä¸­å›½ãƒ»é›²å—çœ", "ä¸­å›½ãƒ»æ–°ç–†ã‚¦ã‚¤ã‚°ãƒ«è‡ªæ²»åŒº", "ä¸­å›½ãƒ»å†…ãƒ¢ãƒ³ã‚´ãƒ«è‡ªæ²»åŒº",
            "ã‚¤ãƒ³ãƒ‰ãƒ»ã‚¦ãƒƒã‚¿ãƒ©ãƒ¼ã‚«ãƒ³ãƒ‰å·", "ã‚¤ãƒ³ãƒ‰ãƒ»ãƒ’ãƒãƒ¼ãƒãƒ£ãƒ«ãƒ»ãƒ—ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ¥å·", "ã‚¤ãƒ³ãƒ‰ãƒ»ãƒãƒãƒ¼ãƒ©ãƒ¼ã‚·ãƒ¥ãƒˆãƒ©å·",
            "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢ãƒ»ã‚«ãƒªãƒãƒ³ã‚¿ãƒ³å³¶", "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢ãƒ»ã‚¹ãƒãƒˆãƒ©å³¶", "ãƒãƒ¬ãƒ¼ã‚·ã‚¢ãƒ»ã‚µãƒå·",
            "ã‚¿ã‚¤ãƒ»ãƒã‚§ãƒ³ãƒã‚¤çœŒ", "ãƒ•ã‚£ãƒªãƒ”ãƒ³ãƒ»ãƒŸãƒ³ãƒ€ãƒŠã‚ªå³¶", "éŸ“å›½ãƒ»æ±ŸåŸé“", "ãƒ¢ãƒ³ã‚´ãƒ«ãƒ»ã‚¦ãƒ©ãƒ³ãƒãƒ¼ãƒˆãƒ«",
            
            # ã‚¢ãƒ•ãƒªã‚«
            "å—ã‚¢ãƒ•ãƒªã‚«ãƒ»è¥¿ã‚±ãƒ¼ãƒ—å·", "å—ã‚¢ãƒ•ãƒªã‚«ãƒ»ã‚¯ãƒ¯ã‚ºãƒ¼ãƒ«ãƒ»ãƒŠã‚¿ãƒ¼ãƒ«å·", "ã‚±ãƒ‹ã‚¢ãƒ»ãƒªãƒ•ãƒˆãƒãƒ¬ãƒ¼å·",
            "ã‚¿ãƒ³ã‚¶ãƒ‹ã‚¢ãƒ»ã‚¢ãƒ«ãƒ¼ã‚·ãƒ£å·", "ãƒ¢ãƒ­ãƒƒã‚³ãƒ»ã‚«ã‚µãƒ–ãƒ©ãƒ³ã‚«ï¼ã‚»ã‚¿ãƒƒãƒˆåœ°æ–¹", "ã‚¢ãƒ«ã‚¸ã‚§ãƒªã‚¢ãƒ»ã‚¢ãƒ«ã‚¸ã‚§çœŒ",
            "ã‚¨ã‚¸ãƒ—ãƒˆãƒ»ã‚«ã‚¤ãƒ­çœŒ", "ãƒŠã‚¤ã‚¸ã‚§ãƒªã‚¢ãƒ»ãƒ©ã‚´ã‚¹å·", "ã‚¬ãƒ¼ãƒŠãƒ»ã‚¢ã‚·ãƒ£ãƒ³ãƒ†ã‚£å·",
            
            # ä¸­æ±
            "ãƒˆãƒ«ã‚³ãƒ»ã‚¢ãƒ³ã‚¿ãƒ«ãƒ¤çœŒ", "ãƒˆãƒ«ã‚³ãƒ»ãƒ ãƒ¼ãƒ©çœŒ", "ã‚¤ã‚¹ãƒ©ã‚¨ãƒ«ãƒ»ãƒã‚¤ãƒ•ã‚¡åœ°åŒº",
            "ãƒ¬ãƒãƒãƒ³ãƒ»å—ãƒ¬ãƒãƒãƒ³çœŒ", "ãƒ¨ãƒ«ãƒ€ãƒ³ãƒ»ã‚¢ãƒ³ãƒãƒ³çœŒ", "ã‚¤ãƒ©ãƒ³ãƒ»ãƒ†ãƒ˜ãƒ©ãƒ³å·",
            
            # ãã®ä»–ã®åœ°åŸŸ
            "ãƒ­ã‚·ã‚¢ãƒ»ã‚·ãƒ™ãƒªã‚¢é€£é‚¦ç®¡åŒº", "ãƒ­ã‚·ã‚¢ãƒ»æ¥µæ±é€£é‚¦ç®¡åŒº", "ãƒ‹ãƒ¥ãƒ¼ã‚¸ãƒ¼ãƒ©ãƒ³ãƒ‰ãƒ»ã‚«ãƒ³ã‚¿ãƒ™ãƒªãƒ¼åœ°æ–¹",
            "ãƒ•ã‚£ã‚¸ãƒ¼ãƒ»ãƒ“ãƒ†ã‚£ãƒ¬ãƒ–å³¶", "ãƒ‘ãƒ—ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚®ãƒ‹ã‚¢ãƒ»ãƒãƒ¼ãƒˆãƒ¢ãƒ¬ã‚¹ãƒ“ãƒ¼"
        ]
        
        # ç«ç½ã‚¿ã‚¤ãƒ—ï¼ˆv0-7ã®60+ã‹ã‚‰80+ã«æ‹¡å¼µï¼‰
        self.fire_types = [
            # æ£®æ—ç«ç½
            "é‡è‘‰æ¨¹æ—ç«ç½", "åºƒè‘‰æ¨¹æ—ç«ç½", "æ··äº¤æ—ç«ç½", "ç«¹æ—ç«ç½", "åŸç”Ÿæ—ç«ç½",
            "äººå·¥æ—ç«ç½", "äºŒæ¬¡æ—ç«ç½", "é˜²é¢¨æ—ç«ç½", "æµ·å²¸æ—ç«ç½", "é«˜å±±æ—ç«ç½",
            
            # è‰åœ°ãƒ»è¾²åœ°ç«ç½
            "è‰åŸç«ç½", "ç‰§è‰åœ°ç«ç½", "è¾²åœ°ç«ç½", "ä¼‘è€•åœ°ç«ç½", "ç•‘ä½œåœ°ç«ç½",
            "æ°´ç”°ç«ç½", "æœæ¨¹åœ’ç«ç½", "èŒ¶ç•‘ç«ç½", "èŠ±ç•‘ç«ç½", "ãƒ“ãƒ‹ãƒ¼ãƒ«ãƒã‚¦ã‚¹ç«ç½",
            
            # éƒ½å¸‚ãƒ»ä½å®…ç«ç½
            "ä½å®…å¯†é›†åœ°ç«ç½", "é«˜å±¤å»ºç¯‰ç«ç½", "å•†æ¥­æ–½è¨­ç«ç½", "å·¥æ¥­åœ°å¸¯ç«ç½", "å€‰åº«ç«ç½",
            "å­¦æ ¡ç«ç½", "ç—…é™¢ç«ç½", "æ–‡åŒ–è²¡ç«ç½", "å®—æ•™æ–½è¨­ç«ç½", "å…¬å…±æ–½è¨­ç«ç½",
            
            # äº¤é€šãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ç«ç½
            "é“è·¯æ²¿ã„ç«ç½", "é‰„é“æ²¿ç·šç«ç½", "é«˜é€Ÿé“è·¯ç«ç½", "ãƒˆãƒ³ãƒãƒ«ç«ç½", "æ©‹æ¢ç«ç½",
            "ç©ºæ¸¯ç«ç½", "æ¸¯æ¹¾ç«ç½", "ç™ºé›»æ‰€ç«ç½", "å¤‰é›»æ‰€ç«ç½", "é€šä¿¡æ–½è¨­ç«ç½",
            
            # è‡ªç„¶ç½å®³é–¢é€£
            "åœ°éœ‡å¾Œç«ç½", "æ´¥æ³¢å¾Œç«ç½", "å°é¢¨é–¢é€£ç«ç½", "è½é›·ç«ç½", "ç«å±±å™´ç«ç«ç½",
            "åœŸç ‚å´©ã‚Œå¾Œç«ç½", "æ´ªæ°´å¾Œç«ç½", "ç«œå·»å¾Œç«ç½", "é›¹å®³å¾Œç«ç½", "å¹²ã°ã¤ç«ç½",
            
            # ç”£æ¥­ãƒ»ç‰¹æ®Šç«ç½
            "çŸ³æ²¹åŒ–å­¦ç«ç½", "ã‚¬ã‚¹æ–½è¨­ç«ç½", "åŒ–å­¦å·¥å ´ç«ç½", "é‡‘å±å·¥å ´ç«ç½", "é£Ÿå“å·¥å ´ç«ç½",
            "ç¹Šç¶­å·¥å ´ç«ç½", "è£½ç´™å·¥å ´ç«ç½", "ãƒªã‚µã‚¤ã‚¯ãƒ«æ–½è¨­ç«ç½", "å»ƒæ£„ç‰©å‡¦ç†å ´ç«ç½", "ä¸‹æ°´å‡¦ç†å ´ç«ç½",
            
            # ç‰¹æ®Šç’°å¢ƒç«ç½
            "æ¹¿åœ°ç«ç½", "ç ‚æ¼ ç«ç½", "æ°·æ²³åœ°å¸¯ç«ç½", "æ´çªŸç«ç½", "åœ°ä¸‹ç«ç½",
            "ç‚­é‰±ç«ç½", "æ²¹ç”°ç«ç½", "ã‚¬ã‚¹ç”°ç«ç½", "å¡©ç”°ç«ç½", "æ¡çŸ³å ´ç«ç½",
            
            # æ–°åˆ†é¡ï¼ˆv1-0è¿½åŠ ï¼‰
            "ãƒã‚¤ã‚ªãƒã‚¹ç«ç½", "å¤ªé™½å…‰ç™ºé›»æ‰€ç«ç½", "é¢¨åŠ›ç™ºé›»æ‰€ç«ç½", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ç«ç½",
            "ç‰©æµã‚»ãƒ³ã‚¿ãƒ¼ç«ç½", "ã‚³ãƒ³ãƒ†ãƒŠç«ç½", "èˆ¹èˆ¶ç«ç½", "èˆªç©ºæ©Ÿç«ç½",
            "å®‡å®™æ–½è¨­ç«ç½", "ç ”ç©¶æ–½è¨­ç«ç½", "æ ¸æ–½è¨­å‘¨è¾ºç«ç½", "è»äº‹æ–½è¨­ç«ç½",
            "è¦³å…‰åœ°ç«ç½", "æ¸©æ³‰åœ°ç«ç½", "ã‚¹ã‚­ãƒ¼å ´ç«ç½", "ã‚´ãƒ«ãƒ•å ´ç«ç½",
            "ã‚­ãƒ£ãƒ³ãƒ—å ´ç«ç½", "å›½ç«‹å…¬åœ’ç«ç½", "å‹•ç‰©åœ’ç«ç½", "æ¤ç‰©åœ’ç«ç½"
        ]

class WildfireDataCollector2M:
    """200ä¸‡æ–‡æ›¸å¯¾å¿œç½å®³æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼"""
    
    def __init__(self, config: Data2MConfig):
        self.config = config
        self.process = psutil.Process()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.stats = {
            "total_generated": 0,
            "chunks_created": 0,
            "start_time": None,
            "sentiment_distribution": {},
            "location_distribution": {},
            "fire_type_distribution": {}
        }
        
        logger.info(f"Initialized 2M Document Collector for v1-0")
        logger.info(f"Target: {config.target_documents:,} documents")
        logger.info(f"Workers: {config.workers}, Batch: {config.batch_size:,}, Chunk: {config.chunk_size:,}")
        logger.info(f"Quality: {len(config.sentiments)} sentiments, {len(config.locations)} locations")

    def _log_memory_usage(self, step_name: str):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ­ã‚°"""
        memory_gb = self.process.memory_info().rss / 1024**3
        logger.info(f"{step_name} - Memory: {memory_gb:.2f}GB")
        
        if memory_gb > self.config.max_memory_gb:
            logger.warning(f"High memory usage: {memory_gb:.2f}GB")
            gc.collect()

    def _generate_enhanced_document(self) -> Dict[str, Any]:
        """v1-0å‘ã‘é«˜å“è³ªæ–‡æ›¸ç”Ÿæˆ"""
        sentiment = random.choice(self.config.sentiments)
        location = random.choice(self.config.locations)
        fire_type = random.choice(self.config.fire_types)
        
        # çµ±è¨ˆæ›´æ–°
        self.stats["sentiment_distribution"][sentiment] = self.stats["sentiment_distribution"].get(sentiment, 0) + 1
        self.stats["location_distribution"][location] = self.stats["location_distribution"].get(location, 0) + 1
        self.stats["fire_type_distribution"][fire_type] = self.stats["fire_type_distribution"].get(fire_type, 0) + 1
        
        # v1-0å‘ã‘é«˜å“è³ªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        templates = [
            f"{location}ã§{fire_type}ãŒç™ºç”Ÿã—ã€ä½æ°‘ã¯{sentiment}ã‚’æ„Ÿã˜ã¦ã„ã‚‹ã€‚æ¶ˆé˜²æ´»å‹•ãŒç¶™ç¶šä¸­ã§ã€é¿é›£æŒ‡ç¤ºãŒå‡ºã•ã‚Œã¦ã„ã‚‹ã€‚",
            f"{fire_type}ã«ã‚ˆã‚Š{location}ã®åºƒç¯„å›²ãŒè¢«å®³ã‚’å—ã‘ã€å¤šãã®äººã€…ãŒ{sentiment}ã®æ„Ÿæƒ…ã‚’æŠ±ã„ã¦ã„ã‚‹ã€‚",
            f"{location}ã§ã®{fire_type}ã«ã‚ˆã‚Šã€åœ°åŸŸä½æ°‘ã¯{sentiment}ã‚’è¦šãˆã€æ”¯æ´æ´»å‹•ã¸ã®å‚åŠ ã‚’è¡¨æ˜ã—ã¦ã„ã‚‹ã€‚",
            f"{fire_type}ã®å½±éŸ¿ã§{location}ã§ã¯é¿é›£ãŒç¶šã„ã¦ãŠã‚Šã€è¢«ç½è€…ã¯{sentiment}ãªæ°—æŒã¡ã§å¾©æ—§ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚",
            f"{location}ã§ç™ºç”Ÿã—ãŸ{fire_type}ã«å¯¾ã—ã€æ•‘åŠ©éšŠã¯{sentiment}ã‚’èƒ¸ã«æ‡¸å‘½ãªæ•‘åŠ©æ´»å‹•ã‚’è¡Œã£ã¦ã„ã‚‹ã€‚",
            f"{fire_type}ã«ã‚ˆã‚‹{location}ã®è¢«å®³çŠ¶æ³ã‚’è¦‹ã¦ã€å¤šãã®å¸‚æ°‘ãŒ{sentiment}ã®æ°—æŒã¡ã‚’å…±æœ‰ã—ã¦ã„ã‚‹ã€‚",
            f"{location}ã§ã®{fire_type}ã«é–¢ã™ã‚‹å ±é“ã‚’å—ã‘ã€å…¨å›½ã‹ã‚‰{sentiment}ã®å£°ã¨æ”¯æ´ãŒå¯„ã›ã‚‰ã‚Œã¦ã„ã‚‹ã€‚",
            f"{fire_type}ã®è„…å¨ã«ã•ã‚‰ã•ã‚ŒãŸ{location}ã®ä½æ°‘ã¯{sentiment}ã‚’ä¹—ã‚Šè¶Šãˆã€çµæŸã‚’å¼·ã‚ã¦ã„ã‚‹ã€‚"
        ]
        
        base_text = random.choice(templates)
        
        # è¤‡é›‘åº¦ãƒ»å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆv0-7æ”¹è‰¯ç‰ˆï¼‰
        complexity_score = min(1.0, (len(base_text) + len(sentiment) + len(location) + len(fire_type)) / 200.0)
        confidence = 0.7 + (complexity_score * 0.3) + random.uniform(-0.1, 0.1)
        confidence = max(0.0, min(1.0, confidence))
        
        # åœ°ç†åº§æ¨™ï¼ˆã‚ˆã‚Šè©³ç´°ï¼‰
        latitude = round(random.uniform(-60, 70), 6)
        longitude = round(random.uniform(-180, 180), 6)
        
        # v1-0è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        severity = random.choice(["è»½å¾®", "ä¸­ç¨‹åº¦", "æ·±åˆ»", "æ¥µã‚ã¦æ·±åˆ»"])
        urgency = random.choice(["ä½", "ä¸­", "é«˜", "æœ€é«˜"])
        
        return {
            "text": base_text,
            "sentiment": sentiment,
            "location": location,
            "fire_type": fire_type,
            "latitude": latitude,
            "longitude": longitude,
            "confidence": round(confidence, 3),
            "complexity_score": round(complexity_score, 3),
            "severity": severity,
            "urgency": urgency,
            "timestamp": f"2025-09-12T{random.randint(0,23):02d}:{random.randint(0,59):02d}:{random.randint(0,59):02d}Z",
            "quality_flags": {
                "high_confidence": confidence > 0.85,
                "high_complexity": complexity_score > 0.7,
                "critical_severity": severity in ["æ·±åˆ»", "æ¥µã‚ã¦æ·±åˆ»"],
                "urgent": urgency in ["é«˜", "æœ€é«˜"]
            }
        }

    def _generate_batch(self, batch_id: int) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒæ–‡æ›¸ç”Ÿæˆ"""
        documents = []
        
        for _ in range(self.config.batch_size):
            doc = self._generate_enhanced_document()
            documents.append(doc)
        
        return documents

    def _save_chunk(self, chunk_data: List[Dict[str, Any]], chunk_id: int) -> str:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        timestamp = time.strftime("%Y%m%d%H%M%S")
        filename = f"2m_wildfire_v1-0_chunk_{chunk_id:03d}_{timestamp}.json"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Saved chunk {chunk_id:03d}: {len(chunk_data):,} docs -> {filename}")
        return str(filepath)

    def generate_2m_dataset(self) -> Dict[str, Any]:
        """200ä¸‡æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        logger.info("=== v1-0: 2M Document Collection Started ===")
        logger.info(f"Target: {self.config.target_documents:,} documents")
        logger.info(f"Configuration: {self.config.workers} workers, {self.config.batch_size:,} batch, {self.config.chunk_size:,} chunk")
        logger.info("Quality settings: Enhanced diversity for v1-0 scaling test")
        
        self.stats["start_time"] = time.time()
        
        total_batches = self.config.target_documents // self.config.batch_size
        chunk_batches = self.config.chunk_size // self.config.batch_size
        
        all_documents = []
        chunk_id = 0
        
        with ThreadPoolExecutor(max_workers=self.config.workers) as executor:
            with tqdm(total=total_batches, desc="Generating 2M documents") as pbar:
                
                # ãƒãƒƒãƒå‡¦ç†ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
                futures = []
                for batch_id in range(total_batches):
                    future = executor.submit(self._generate_batch, batch_id)
                    futures.append(future)
                
                # çµæœåé›†ã¨ãƒãƒ£ãƒ³ã‚¯ä¿å­˜
                for i, future in enumerate(as_completed(futures)):
                    try:
                        batch_documents = future.result()
                        all_documents.extend(batch_documents)
                        
                        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ä¿å­˜
                        if len(all_documents) >= self.config.chunk_size:
                            chunk_documents = all_documents[:self.config.chunk_size]
                            remaining_documents = all_documents[self.config.chunk_size:]
                            
                            self._save_chunk(chunk_documents, chunk_id)
                            chunk_id += 1
                            self.stats["chunks_created"] += 1
                            
                            all_documents = remaining_documents
                            
                            # ãƒ¡ãƒ¢ãƒªç®¡ç†
                            if chunk_id % 5 == 0:
                                self._log_memory_usage(f"chunk_{chunk_id}")
                                gc.collect()
                        
                        # é€²æ—æ›´æ–°
                        pbar.update(1)
                        self.stats["total_generated"] = (i + 1) * self.config.batch_size
                        
                        # é€²æ—ãƒ­ã‚°ï¼ˆ50ä¸‡æ–‡æ›¸æ¯ï¼‰
                        if self.stats["total_generated"] % 500000 == 0:
                            elapsed = time.time() - self.stats["start_time"]
                            speed = self.stats["total_generated"] / elapsed
                            memory_gb = self.process.memory_info().rss / 1024**3
                            logger.info(f"Progress: {self.stats['total_generated']:,}/{self.config.target_documents:,} documents, Memory: {memory_gb:.2f}GB")
                    
                    except Exception as e:
                        logger.error(f"Batch generation error: {e}")
                        continue
        
        # æ®‹ã‚Šã®æ–‡æ›¸ã‚’æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ä¿å­˜
        if all_documents:
            self._save_chunk(all_documents, chunk_id)
            self.stats["chunks_created"] += 1
        
        # æœ€çµ‚çµ±è¨ˆ
        end_time = time.time()
        total_time = end_time - self.stats["start_time"]
        speed = self.config.target_documents / total_time
        
        logger.info("=== v1-0: 2M Document Collection Completed ===")
        logger.info(f"Generated: {self.config.target_documents:,} documents")
        logger.info(f"Time: {total_time:.1f}s ({total_time/60:.1f}min)")
        logger.info(f"Speed: {speed:.1f} docs/sec")
        logger.info(f"Chunks: {self.stats['chunks_created']} files")
        
        memory_gb = self.process.memory_info().rss / 1024**3
        logger.info(f"Memory: {memory_gb:.2f}GB peak")
        logger.info(f"Output: {self.config.output_dir}/")
        
        return {
            "total_documents": self.config.target_documents,
            "total_time": total_time,
            "speed": speed,
            "chunks_created": self.stats["chunks_created"],
            "memory_peak_gb": memory_gb,
            "sentiment_distribution": dict(list(self.stats["sentiment_distribution"].items())[:10]),
            "location_distribution": dict(list(self.stats["location_distribution"].items())[:10]),
            "fire_type_distribution": dict(list(self.stats["fire_type_distribution"].items())[:10])
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    config = Data2MConfig()
    collector = WildfireDataCollector2M(config)
    
    try:
        results = collector.generate_2m_dataset()
        
        print(f"\nğŸ‰ 2M Document Collection v1-0 completed!")
        print(f"ğŸ“Š Generated: {results['total_documents']:,} documents")
        print(f"â±ï¸  Time: {results['total_time']:.1f}s ({results['total_time']/60:.1f}min)")
        print(f"ğŸš€ Speed: {results['speed']:.1f} docs/sec")
        print(f"ğŸ“ Chunks: {results['chunks_created']} files")
        print(f"ğŸ’¾ Memory: {results['memory_peak_gb']:.2f}GB")
        print(f"ğŸ“‚ Output: {config.output_dir}")
        print(f"ğŸ¯ Quality: Enhanced diversity for v1-0 scaling test")
        
    except Exception as e:
        logger.error(f"Data collection failed: {e}")
        raise

if __name__ == "__main__":
    main()