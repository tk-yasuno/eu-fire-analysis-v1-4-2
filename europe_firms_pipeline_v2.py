#!/usr/bin/env python3
"""
ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘åœ°åŸŸ NASA FIRMS æ£®æ—ç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ãƒ•ãƒ«EUãƒ»UKãƒ»åŒ—æ¬§ã‚’ã‚«ãƒãƒ¼ã™ã‚‹ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã‚¨ãƒªã‚¢ã®ç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 
çµæœã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

ã‚¨ãƒªã‚¢ç¯„å›²:
- ç·¯åº¦: åŒ—ç·¯34Â°ï½åŒ—ç·¯72Â° (åœ°ä¸­æµ·ï½åŒ—æ¥µåœ)  
- çµŒåº¦: è¥¿çµŒ25Â°ï½æ±çµŒ50Â° (å¤§è¥¿æ´‹ï½ã‚¦ãƒ©ãƒ«)
- å¯¾è±¡å›½: ã‚¤ã‚¿ãƒªã‚¢ï¼ˆfocusï¼‰ã€ãƒ•ãƒ©ãƒ³ã‚¹ã€ã‚¹ãƒšã‚¤ãƒ³ã€ãƒ‰ã‚¤ãƒ„ã€è‹±å›½ã€åŒ—æ¬§è«¸å›½ã€å…¨EUåŠ ç›Ÿå›½
"""

import os
import sys
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional
from typing import Dict, List, Optional

# ãƒ‘ã‚¹è¨­å®š
scripts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'scripts')
sys.path.append(scripts_dir)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scripts.data_collector import DataCollector
from scripts.model_loader import ModelLoader
from scripts.embedding_generator import EmbeddingGenerator
from adaptive_clustering_selector import AdaptiveClusteringSelector
from scripts.visualization import VisualizationManager
from cluster_feature_analyzer import ClusterFeatureAnalyzer
from fire_analysis_report_generator import FireAnalysisReportGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)


def _time_step(step_name):
    """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“æ¸¬å®šç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            start_time = time.time()
            logger.info(f"=== Starting: {step_name} ===")
            try:
                result = func(self, *args, **kwargs)
                end_time = time.time()
                logger.info(f"=== Completed: {step_name} ({end_time - start_time:.2f}s) ===")
                return result
            except Exception as e:
                end_time = time.time()
                logger.error(f"=== Failed: {step_name} ({end_time - start_time:.2f}s) - {e} ===")
                raise
        return wrapper
    return decorator


class EuropeFirmsAnalyzer:
    """ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘åœ°åŸŸNASA FIRMSæ£®æ—ç«ç½åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str = "config_europe_firms.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime(self.config['output']['timestamp_format'])
        self.output_dir = f"data_firms_europe_{timestamp}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info(f"Output directory: {self.output_dir}")
        
        # å‡¦ç†æ™‚é–“è¨˜éŒ²
        self.step_times = {}
        
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"Configuration loaded: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise
    
    def _time_step(self, step_name: str):
        """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“æ¸¬å®šãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                elapsed = time.time() - start_time
                self.step_times[step_name] = elapsed
                logger.info(f"Step '{step_name}' completed in {elapsed:.2f}s")
                return result
            return wrapper
        return decorator
    
    def run_pipeline(self) -> str:
        """
        ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘åœ°åŸŸç«ç½åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        
        Returns:
            çµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        logger.info("ï¿½ Starting Europe FIRMS Fire Analysis Pipeline")
        
        start_time = time.time()
        
        try:
            # Step 1: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self._initialize_components()
            
            # Step 2: NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›†
            data = self._collect_nasa_firms_data()
            if data is None or len(data) == 0:
                logger.warning("No data collected - aborting pipeline")
                return None
            
            # Step 3: åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embeddings, scores = self._generate_embeddings(data)
            
            # Step 4: é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            labels, clustering_results = self._perform_adaptive_clustering(embeddings)
            
            # Step 5: åŒ…æ‹¬çš„å¯è¦–åŒ–
            self._create_comprehensive_visualizations(embeddings, labels, data, scores)
            
            # Step 6: ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹å¾´åˆ†æ
            feature_analysis = self._perform_cluster_feature_analysis(data, labels)
            
            # Step 7: æœ€çµ‚çµæœä¿å­˜
            result_path = self._save_final_results(data, labels, clustering_results, feature_analysis)
            
            # Step 8: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_comprehensive_report(data, labels, clustering_results, feature_analysis)
            
            # å®Ÿè¡Œå®Œäº†
            total_time = time.time() - start_time
            logger.info(f"ğŸ‰ Europe Fire Analysis Pipeline completed successfully!")
            logger.info(f"Processing time: {total_time:.2f}s for {len(data)} samples")
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self._display_results_summary(clustering_results, total_time, len(data))
            
            return result_path
            
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            raise
    
    def _initialize_components(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        logger.info("=== Initializing Pipeline Components ===")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
        self.model_loader = ModelLoader(
            model_name=self.config['embedding']['model_name'],
            device=self.config['embedding']['device']
        )
        self.model = self.model_loader.load_model()
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå™¨åˆæœŸåŒ–
        self.embedding_generator = EmbeddingGenerator(
            model=self.model,
            batch_size=self.config['embedding']['batch_size']
        )
        
        # é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é¸æŠå™¨åˆæœŸåŒ–
        clustering_config = self.config['adaptive_clustering']
        self.clustering_selector = AdaptiveClusteringSelector(
            output_dir=self.output_dir,
            min_cluster_quality=clustering_config['quality_thresholds']['min_cluster_quality'],
            max_noise_ratio=clustering_config['quality_thresholds']['max_noise_ratio']
        )
        
        # å¯è¦–åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        self.visualization_manager = VisualizationManager(
            output_dir=self.output_dir,
            figsize=(12, 8)
        )
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹å¾´åˆ†æå™¨åˆæœŸåŒ–
        self.feature_analyzer = ClusterFeatureAnalyzer(
            output_dir=self.output_dir
        )
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–
        self.report_generator = FireAnalysisReportGenerator(
            output_dir=self.output_dir,
            config=self.config
        )
        
        logger.info("All components initialized successfully")
    
    def _collect_nasa_firms_data(self):
        """NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== Collecting NASA FIRMS Data (Europe Region) ===")
        
        collector = DataCollector(
            raw_data_dir=os.path.join(self.output_dir, "raw"),
            cleaned_data_dir=os.path.join(self.output_dir, "cleaned")
        )
        
        # å—ç±³åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿åé›†
        nasa_config = self.config['nasa_firms']
        data = collector.collect_nasa_firms_data(
            map_key=nasa_config['map_key'],
            area_params=nasa_config['area_params'],
            days_back=nasa_config['days_back'],
            satellite=nasa_config['satellite']
        )
        
        if data is not None:
            # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            confidence_threshold = self.config['nasa_firms']['confidence_threshold']
            high_confidence_data = data[data['confidence'] >= confidence_threshold]
            
            logger.info(f"Filtered to {len(high_confidence_data)} high-confidence detections (>= {confidence_threshold}%)")
            
            # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
            max_samples = self.config['processing']['max_samples']
            if len(high_confidence_data) > max_samples:
                logger.warning(f"Data exceeds max_samples limit ({len(high_confidence_data)} > {max_samples})")
                logger.warning(f"Truncating to first {max_samples} samples for system stability")
                high_confidence_data = high_confidence_data.head(max_samples)
            
            logger.info(f"Final dataset: {len(high_confidence_data)} NASA FIRMS records for comprehensive analysis")
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            data_path = os.path.join(self.output_dir, "nasa_firms_data.csv")
            high_confidence_data.to_csv(data_path, index=False)
            logger.info(f"Data saved: {data_path}")
            
            return high_confidence_data
        else:
            logger.error("Failed to collect NASA FIRMS data")
            return None
    
    def _generate_embeddings(self, data):
        """ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        logger.info("=== Generating Text Embeddings ===")
        
        # NASA FIRMSãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç«ç½æ¤œçŸ¥æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        texts = []
        for _, row in data.iterrows():
            # ç«ç½æ¤œçŸ¥æƒ…å ±ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚’ç”Ÿæˆ
            text = f"Fire detection at latitude {row['latitude']:.3f}, longitude {row['longitude']:.3f} on {row['acq_date']} at {row['acq_time']}. " \
                   f"Brightness temperature: {row['bright_ti4']:.2f}K, confidence: {row['confidence']}%, " \
                   f"Fire radiative power: {row['frp']:.2f}MW. Satellite: {row['satellite']}-{row['instrument']}, " \
                   f"Day/Night: {'Daytime' if row['daynight'] == 'D' else 'Nighttime'} detection."
            texts.append(text)
        
        logger.info(f"Generated {len(texts)} fire detection text descriptions")
        
        embeddings, scores = self.embedding_generator.generate_embeddings_batch(texts)
        
        if embeddings is not None:
            logger.info(f"Generated {len(embeddings)} embeddings (dim: {embeddings.shape[1]})")
            
            # åŸ‹ã‚è¾¼ã¿ä¿å­˜
            import numpy as np
            embeddings_path = os.path.join(self.output_dir, "embeddings.npy")
            np.save(embeddings_path, embeddings.cpu().numpy())
            logger.info(f"Embeddings saved: {embeddings_path}")
            
            return embeddings, scores
        else:
            raise ValueError("Failed to generate embeddings")
    
    def _perform_adaptive_clustering(self, embeddings):
        """é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        logger.info("=== Performing Adaptive Clustering ===")
        
        # é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        hdbscan_params = self.config['adaptive_clustering']['hdbscan_params']
        kmeans_params = self.config['adaptive_clustering']['kmeans_params']
        
        logger.info(f"Adaptive parameters: HDBSCAN {hdbscan_params}, k-means {kmeans_params}")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        best_result, selection_details = self.clustering_selector.select_best_clustering(
            embeddings=embeddings.cpu().numpy(),
            hdbscan_params=hdbscan_params,
            kmeans_params=kmeans_params
        )
        
        logger.info(f"Selected method: {best_result.method}")
        logger.info(f"Selection reason: {selection_details['selection_reason']}")
        
        clustering_results = {
            'labels': best_result.labels,
            'quality_score': best_result.metrics.quality_score,
            'selected_method': best_result.method,
            'method_reason': selection_details['selection_reason'],
            'n_clusters': best_result.metrics.n_clusters,
            'noise_ratio': best_result.metrics.noise_ratio,
            'metrics': best_result.metrics
        }
        
        return best_result.labels, clustering_results
    
    def _create_comprehensive_visualizations(self, embeddings, labels, data, scores):
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ä½œæˆ"""
        logger.info("=== Creating Comprehensive Visualizations ===")
        
        try:
            # t-SNEæ¬¡å…ƒå‰Šæ¸›
            coords_2d = self.visualization_manager.reduce_dimensions_tsne(embeddings.cpu().numpy())
            
            # numpyé…åˆ—ã«å¤‰æ›
            scores_np = scores.cpu().numpy() if hasattr(scores, 'cpu') else scores
            
            # t-SNEã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å¯è¦–åŒ–
            tsne_plot_path = self.visualization_manager.create_cluster_plot(
                coords=coords_2d,
                labels=labels,
                scores=scores_np,
                title="Europe Fire Detection Clusters (t-SNE)"
            )
            
            # ã‚¹ã‚³ã‚¢åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ
            score_plot_path = self.visualization_manager.create_score_distribution_plot(
                scores=scores_np,
                labels=labels
            )
            
            logger.info(f"âœ… Generated t-SNE plot: {tsne_plot_path}")
            logger.info(f"âœ… Generated score distribution: {score_plot_path}")
            
            return {
                'tsne_plot': tsne_plot_path,
                'score_plot': score_plot_path,
                'coords_2d': coords_2d
            }
                
        except Exception as e:
            logger.error(f"Visualization failed: {e}")
            return {}
    
    def _perform_cluster_feature_analysis(self, data, labels):
        """ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹å¾´åˆ†æå®Ÿè¡Œ"""
        logger.info("=== Performing Cluster Feature Analysis ===")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹å¾´åˆ†æå®Ÿè¡Œ
        feature_analysis = self.feature_analyzer.analyze_cluster_features(data, labels)
        
        # ç‰¹å¾´åˆ†æå¯è¦–åŒ–ä½œæˆ
        viz_files = self.feature_analyzer.create_feature_visualizations(feature_analysis, self.output_dir)
        
        logger.info(f"âœ… Generated {len(viz_files)} cluster feature analysis visualizations")
        for viz_file in viz_files:
            logger.info(f"  ğŸ“ˆ Feature viz: {viz_file}")
        
        return feature_analysis
    
    def _save_final_results(self, data, labels, clustering_results, feature_analysis):
        """æœ€çµ‚çµæœä¿å­˜"""
        logger.info("=== Saving Final Results ===")
        
        # æœ€çµ‚çµæœJSONä¿å­˜ï¼ˆnumpyé…åˆ—ã‚’é™¤å¤–ï¼‰
        clustering_summary = {
            'quality_score': clustering_results['quality_score'],
            'selected_method': clustering_results['selected_method'],
            'method_reason': clustering_results['method_reason'],
            'n_clusters': clustering_results['n_clusters'],
            'noise_ratio': clustering_results['noise_ratio']
        }
        
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'region': 'Europe',
            'total_samples': len(data),
            'clustering': clustering_summary,
            'feature_analysis_summary': {
                'total_clusters': len(feature_analysis.get('clusters', {})),
                'geographic_distribution': len(feature_analysis.get('geographic_distribution', {})),
                'temporal_patterns': len(feature_analysis.get('temporal_patterns', {}))
            },
            'config_used': self.config_path
        }
        
        result_path = os.path.join(self.output_dir, "final_europe_results.json")
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Final results saved: {result_path}")
        
        # ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ä¿å­˜
        data_with_labels = data.copy()
        data_with_labels['cluster_label'] = labels
        
        labeled_data_path = os.path.join(self.output_dir, "europe_fires_clustered.csv")
        data_with_labels.to_csv(labeled_data_path, index=False)
        logger.info(f"Labeled data saved: {labeled_data_path}")
        
        return result_path
    
    def _generate_comprehensive_report(self, data, labels, clustering_results, feature_analysis):
        """åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("=== Generating Comprehensive Analysis Report ===")
        
        try:
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ‡ãƒ¼ã‚¿æº–å‚™
            report_data = {
                'data': data,
                'labels': labels,
                'clustering_results': clustering_results,
                'feature_analysis': feature_analysis,
                'region': 'Europe',
                'region_name': 'Europe',
                'focus_country': 'Italy'
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Ÿè¡Œ
            report_path = self.report_generator.generate_report(report_data)
            
            logger.info(f"ğŸ“ Comprehensive analysis report generated: {report_path}")
            
            return report_path
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            logger.info("Pipeline completed successfully despite report generation issue")
            return None
    
    def _display_results_summary(self, clustering_results, total_time, num_samples):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*70)
        print("ï¿½ EUROPE FOREST FIRE ANALYSIS RESULTS")
        print("="*70)
        print(f"âœ… Status: SUCCESS")
        print(f"ğŸ¯ Selected Method: {clustering_results['selected_method']}")
        print(f"ğŸ“Š Quality Score: {clustering_results['quality_score']:.3f}")
        print(f"ğŸ”¢ Clusters Found: {clustering_results['n_clusters']}")
        print(f"ğŸ“‰ Noise Ratio: {clustering_results['noise_ratio']:.1%}")
        print(f"ğŸ“¦ Total Fire Detections: {num_samples}")
        print(f"â±ï¸ Processing Time: {total_time:.2f}s")
        print(f"ğŸ“ Results Directory: {self.output_dir}")
        print(f"ğŸŒ Region Coverage: Europe (34Â°N-72Â°N, 25Â°W-50Â°E)")
        print("="*70)


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¨­å®š
    parser = argparse.ArgumentParser(description='Europe Fire Detection Analysis Pipeline')
    parser.add_argument('--config', default='config_europe_firms.json',
                       help='Configuration file path (default: config_europe_firms.json)')
    args = parser.parse_args()
    
    try:
        # æŒ‡å®šã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§åˆ†æå®Ÿè¡Œ
        analyzer = EuropeFirmsAnalyzer(config_path=args.config)
        result_path = analyzer.run_pipeline()
        
        if result_path:
            print(f"\nğŸ‰ Analysis completed successfully!")
            print(f"ğŸ“ Results saved in: {analyzer.output_dir}")
            print(f"ğŸ“„ Main results file: {result_path}")
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
            import os
            output_files = os.listdir(analyzer.output_dir)
            
            print(f"\nğŸ“Š Generated files ({len(output_files)} total):")
            for file in sorted(output_files):
                if file.endswith('.png'):
                    print(f"  ğŸ–¼ï¸  {file}")
                elif file.endswith('.json'):
                    print(f"  ğŸ“‹ {file}")
                elif file.endswith('.csv'):
                    print(f"  ğŸ“Š {file}")
                elif file.endswith('.md'):
                    print(f"  ğŸ“ {file} (ğŸ“– COMPREHENSIVE ANALYSIS REPORT)")
                else:
                    print(f"  ğŸ“„ {file}")
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹åˆ¥æ¡ˆå†…
            report_file = os.path.join(analyzer.output_dir, "comprehensive_fire_analysis_report.md")
            if os.path.exists(report_file):
                print(f"\nğŸ“– **åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ**")
                print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")
                print(f"   å†…å®¹: 6ã¤ã®å›³è¡¨ã‚’ç”¨ã„ãŸè©³ç´°ãªç«ç½åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
                print(f"   å½¢å¼: Markdownå½¢å¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§é–²è¦§å¯èƒ½ï¼‰")
        else:
            print("âŒ Analysis failed - no data collected")
            
    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        raise


if __name__ == "__main__":
    main()